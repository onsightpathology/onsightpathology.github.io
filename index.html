<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OnSight: A Real-Time Computational Pathology Companion</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600;700&family=Source+Sans+Pro:wght@300;400;600&display=swap');

        :root {
            --primary-bg: #0d1117;
            --secondary-bg: #161b22;
            --accent-blue: #3B82F6;
            --accent-blue-hover: #2563EB;
            --accent-purple: #8B5CF6;
            --text-primary: #e6edf3;
            --text-secondary: #7d8590;
            --border-color: #30363d;
            --heading-font: 'Montserrat', sans-serif;
            --body-font: 'Source Sans Pro', sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }
        html { scroll-behavior: smooth; }
        body {
            font-family: var(--body-font);
            background-color: var(--primary-bg);
            color: var(--text-primary);
            line-height: 1.8;
            font-weight: 300;
            font-size: 1.1rem;
        }

        .container { max-width: 1200px; margin: 0 auto; padding: 0 30px; }

        h1, h2, h3, h4 { font-family: var(--heading-font); font-weight: 600; line-height: 1.3; }
        h1 { font-size: 3.5rem; color: #fff; }
        h2 { font-size: 2.5rem; color: #fff; text-align: center; margin-bottom: 40px; padding-bottom: 20px; border-bottom: 1px solid var(--border-color); }
        h3 { font-size: 1.8rem; color: var(--accent-blue); margin-bottom: 15px; }
        h4 { font-size: 1.2rem; color: var(--text-primary); margin-bottom: 10px; font-weight: 600; }
        
        p { margin-bottom: 1.2rem; max-width: 800px; }
        .center-p { margin-left: auto; margin-right: auto; text-align: center; }

        section { padding: 80px 0; }
        .section-bg { background-color: var(--secondary-bg); border-top: 1px solid var(--border-color); border-bottom: 1px solid var(--border-color); }

        /* --- Navigation Bar --- */
        .navbar {
            background: rgba(13, 17, 23, 0.8);
            backdrop-filter: blur(10px);
            padding: 1rem 0;
            position: fixed;
            width: 100%;
            top: 0;
            z-index: 1000;
            border-bottom: 1px solid var(--border-color);
        }
        .navbar .container { display: flex; justify-content: space-between; align-items: center; }
        .navbar .logo { font-family: var(--heading-font); font-size: 1.5rem; color: #fff; text-decoration: none; font-weight: 700; }
        .nav-links { list-style: none; display: flex; gap: 25px; }
        .nav-links a { color: var(--text-primary); text-decoration: none; font-weight: 400; transition: color 0.3s ease; }
        .nav-links a:hover { color: var(--accent-blue); }

        /* --- Hero Section --- */
        .hero {
            height: 100vh;
            display: flex;
            align-items: center;
            text-align: left;
            position: relative;
            background-image: linear-gradient(rgba(13, 17, 23, 0.95), rgba(13, 17, 23, 0.8)), url('https://i.imgur.com/G5aR2iC.jpeg'); /* Placeholder pathology background */
            background-size: cover;
            background-position: center;
        }
        .hero-content { animation: fadeIn 2s ease-in-out; }
        .hero .subtitle { font-size: 1.4rem; font-weight: 300; max-width: 750px; margin: 1rem 0 2.5rem 0; color: var(--text-secondary); }
        .btn { display: inline-block; padding: 15px 35px; border-radius: 5px; text-decoration: none; font-family: var(--heading-font); font-weight: 600; transition: all 0.3s ease; cursor: pointer; border: none; font-size: 1.1rem; }
        .btn-primary { background-color: var(--accent-blue); color: #fff; }
        .btn-primary:hover { background-color: var(--accent-blue-hover); transform: translateY(-3px); box-shadow: 0 8px 20px rgba(59, 130, 246, 0.25); }
        .btn-secondary { background: transparent; color: var(--text-primary); border: 2px solid var(--border-color); margin-left: 15px; }
        .btn-secondary:hover { background-color: var(--accent-blue); border-color: var(--accent-blue); }
        
        /* --- Core Technology Section --- */
        .workflow-grid { display: grid; grid-template-columns: repeat(3, 1fr); gap: 40px; align-items: center; text-align: center; margin-top: 50px; }
        .workflow-step { position: relative; }
        .workflow-step .icon { font-size: 4rem; margin-bottom: 1rem; color: var(--accent-purple); }
        .workflow-step h4 { color: #fff; }
        .workflow-step::after { content: '‚Üí'; font-size: 3rem; color: var(--border-color); position: absolute; right: -35px; top: 30%; transform: translateY(-50%); }
        .workflow-step:last-child::after { content: ''; }

        /* --- Capabilities Section --- */
        .capabilities-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(320px, 1fr)); gap: 30px; }
        .capability-card { background: var(--secondary-bg); padding: 30px; border-radius: 8px; border: 1px solid var(--border-color); }
        .capability-card .icon { font-size: 2.5rem; margin-bottom: 1rem; color: var(--accent-blue); }
        .capability-card h3 { font-size: 1.3rem; margin-bottom: 10px; color: #fff;}
        .capability-card p { color: var(--text-secondary); font-size: 1rem; }

        /* --- Applications Section --- */
        .application-grid { display: grid; grid-template-columns: 1fr 1.2fr; gap: 50px; align-items: center; margin-bottom: 60px; }
        .application-grid:nth-child(even) .application-text { grid-column: 2; }
        .application-grid:nth-child(even) .application-figures { grid-column: 1; grid-row: 1; }
        .application-text ul { list-style-position: inside; padding-left: 10px; margin-bottom: 1.2rem; }
        .application-figures { display: grid; grid-template-columns: 1fr 1fr; gap: 15px; }
        .figure-placeholder { background: #000; border-radius: 8px; aspect-ratio: 1 / 1; display: flex; flex-direction: column; align-items: center; justify-content: center; color: #555; text-align: center; padding: 10px; font-style: italic; border: 2px dashed var(--border-color); }
        .figure-placeholder.large { grid-column: 1 / -1; aspect-ratio: 16 / 9; }

        /* --- Research & Team Section --- */
        .team-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; text-align: center; margin-top: 40px; }
        .team-member { background: var(--primary-bg); padding: 20px; border-radius: 8px; }
        .team-member .name { font-weight: 600; color: #fff; }
        .team-member .affiliation { font-size: 0.9rem; color: var(--text-secondary); }
        #publication blockquote { max-width: 800px; margin: 40px auto; font-style: italic; font-size: 1.2rem; border-left: 4px solid var(--accent-purple); padding-left: 25px; text-align: left; }

        /* --- Footer --- */
        footer { text-align: center; padding: 40px 0; font-size: 0.9rem; background-color: var(--secondary-bg); color: var(--text-secondary); border-top: 1px solid var(--border-color); }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(20px); } to { opacity: 1; transform: translateY(0); } }

        /* --- Responsive Design --- */
        @media(max-width: 992px) {
            .application-grid, .application-grid:nth-child(even) { grid-template-columns: 1fr; }
            .application-grid:nth-child(even) .application-text, .application-grid:nth-child(even) .application-figures { grid-column: auto; grid-row: auto; }
        }
        @media(max-width: 768px) {
            h1 { font-size: 2.8rem; }
            h2 { font-size: 2rem; }
            .navbar .container { flex-direction: column; gap: 10px; }
            .workflow-grid { grid-template-columns: 1fr; }
            .workflow-step::after { display: none; }
        }

    </style>
</head>
<body>

    <nav class="navbar">
        <div class="container">
            <a href="#" class="logo">OnSight</a>
            <ul class="nav-links">
                <li><a href="#challenge">The Challenge</a></li>
                <li><a href="#technology">Technology</a></li>
                <li><a href="#applications">Applications</a></li>
                <li><a href="#research">Research</a></li>
            </ul>
        </div>
    </nav>

    <header class="hero">
        <div class="container">
            <div class="hero-content">
                <h1>OnSight</h1>
                <p class="subtitle">A Real-Time Computational Pathology Companion for Histopathology. OnSight delivers on-screen, AI-driven histological analysis, removing critical barriers to the adoption of machine learning in clinical and research workflows.</p>
                <a href="https://drive.google.com/drive/folders/12OlcrfU7RBHSfTWlAusk9DsoY3P-waey" target="_blank" class="btn btn-primary">Download Executable</a>
                <a href="#applications" class="btn btn-secondary">View Applications</a>
            </div>
        </div>
    </header>

    <main>
        <section id="challenge">
            <div class="container">
                <h2>The Challenge in Computational Pathology</h2>
                <p class="center-p">The histopathological examination of tissue remains a cornerstone of modern disease classification. However, the qualitative nature of this process is prone to subjective variability. While deep learning has shown immense promise in objectifying analysis, widespread adoption is stalled by significant technological and logistical barriers.</p>
                <div class="capabilities-grid" style="margin-top: 40px;">
                    <div class="capability-card">
                        <div class="icon">üß©</div>
                        <h3>Integration Complexity</h3>
                        <p>Diverse scanner manufacturers, proprietary WSI formats, and varied viewing software create a fragmented ecosystem, complicating the deployment of AI solutions.</p>
                    </div>
                    <div class="capability-card">
                        <div class="icon">‚òÅÔ∏è</div>
                        <h3>Data Privacy & Security</h3>
                        <p>Online platforms and API-based solutions often require the transfer of sensitive patient data, raising significant privacy and data governance concerns.</p>
                    </div>
                    <div class="capability-card">
                        <div class="icon">üíª</div>
                        <h3>Accessibility & Cost</h3>
                        <p>Many solutions require specialized hardware, complex software dependencies, or significant coding expertise, limiting access for many pathology centers.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="technology" class="section-bg">
            <div class="container">
                <h2>Our Solution: A Vendor-Agnostic Workflow</h2>
                <p class="center-p">OnSight is a freely accessible, self-contained software that operates directly on the user's personal computer. By leveraging customized, continuous screen capture, it provides real-time AI inferences without requiring any integration with underlying pathology software or transfer of image data.</p>
                <div class="workflow-grid">
                    <div class="workflow-step">
                        <div class="icon">üñºÔ∏è</div>
                        <h4>1. Define Region of Interest</h4>
                        <p>The user defines a capture area on their display corresponding to the tissue view in their preferred WSI software.</p>
                    </div>
                    <div class="workflow-step">
                        <div class="icon">üß†</div>
                        <h4>2. Real-Time Local Inference</h4>
                        <p>OnSight continuously feeds captured frames to a local AI engine, running selected models on the user's hardware (CPU/GPU).</p>
                    </div>
                    <div class="workflow-step">
                        <div class="icon">üìä</div>
                        <h4>3. Visualize & Aggregate Results</h4>
                        <p>Inferences (classifications, bounding boxes, masks) are overlaid in real-time. Results can be aggregated and calibrated for quantitative analysis.</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="applications">
            <div class="container">
                <h2>Validated Applications</h2>
                <p class="center-p">OnSight is packaged with models targeting routine histopathological tasks, validated on both internal clinical datasets and external public cohorts to demonstrate generalizability.</p>

                <div class="application-grid" style="margin-top: 50px;">
                    <div class="application-text">
                        <h3>Brain Tumor Classification</h3>
                        <p>Guides initial differential diagnosis of common CNS neoplasms from H&E-stained sections. This module helps differentiate the four most common tumor patterns encountered in neuropathology.</p>
                        <ul>
                            <li><strong>Model:</strong> Vision Transformer (ViT-B/16) fine-tuned on 80,000+ internal tiles.</li>
                            <li><strong>Task:</strong> Classifies glial, meningiothelial, epithelial, and schwannian histologies.</li>
                            <li><strong>Output:</strong> Real-time probability scores for each class displayed on the GUI.</li>
                        </ul>
                    </div>
                    <div class="application-figures">
                        <div class="figure-placeholder">GIF showing real-time classification overlay as user pans across a glioma.</div>
                        <div class="figure-placeholder">Confusion matrix of the model's performance on a validation dataset.</div>
                    </div>
                </div>

                <div class="application-grid">
                    <div class="application-text">
                        <h3>Mitosis Detection & Quantification</h3>
                        <p>Objectifies the critical task of identifying mitotic figures for tumor grading. Users can dynamically adjust the confidence threshold to manage the sensitivity/specificity trade-off in real-time.</p>
                        <ul>
                            <li><strong>Model:</strong> RetinaNet with a ResNet-50 FPN backbone, trained on the MIDOG++ dataset.</li>
                            <li><strong>Task:</strong> Detects mitotic figures with bounding boxes.</li>
                            <li><strong>Output:</strong> Calibrated mitotic counts (e.g., mitoses per mm¬≤), aiding in reproducible tumor grading.</li>
                        </ul>
                    </div>
                    <div class="application-figures">
                        <div class="figure-placeholder">GIF showing mitosis bounding boxes appearing in real-time on a meningioma.</div>
                        <div class="figure-placeholder">Receiver Operating Characteristic (ROC) curve showing model performance.</div>
                    </div>
                </div>

                <div class="application-grid">
                    <div class="application-text">
                        <h3>Immunohistochemistry (IHC) Quantification</h3>
                        <p>Provides rapid and reproducible quantification of IHC stains, such as the Ki-67 proliferation index. The model is robust to variations in staining and contrast.</p>
                        <ul>
                            <li><strong>Model:</strong> YOLO-based instance segmentation architecture.</li>
                            <li><strong>Task:</strong> Segments and differentiates Ki-67 positive and negative nuclei.</li>
                            <li><strong>Output:</strong> Real-time proliferation index (Ki-67 %) with adjustable transparency for visualizing segmentation masks.</li>
                        </ul>
                    </div>
                    <div class="application-figures">
                         <div class="figure-placeholder large">Side-by-side comparison: OnSight quantification vs. manual QuPath annotation.</div>
                         <div class="figure-placeholder">Image showing segmentation overlay with transparency control.</div>
                         <div class="figure-placeholder">Correlation plot of OnSight vs. expert pathologist Ki-67 scores.</div>
                    </div>
                </div>
                
                <div class="application-grid">
                    <div class="application-text">
                        <h3>Interpretable AI with Vision-Language Chat</h3>
                        <p>Enhances understanding of the AI's decision-making process through an integrated, conversational interface, fostering trust and providing deeper insights.</p>
                        <ul>
                            <li><strong>Model:</strong> HuatuoGPT-7B, a vision-language model optimized for medical applications.</li>
                            <li><strong>Task:</strong> Answers natural language questions about the on-screen image.</li>
                            <li><strong>Example Query:</strong> "What specific cytoarchitectural pattern makes this glial?"</li>
                        </ul>
                    </div>
                    <div class="application-figures">
                        <div class="figure-placeholder large">Screenshot of the OnSight GUI showing the image viewer and the interactive chat pane side-by-side.</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="research" class="section-bg">
            <div class="container" id="publication">
                <h2>The Research & Team</h2>
                <p class="center-p">OnSight is the result of a collaborative effort at leading Canadian institutions, designed to translate state-of-the-art machine learning research into a practical tool for pathologists.</p>
                <blockquote>"We believe OnSight removes numerous critical barriers to facilitate accelerated adoption of machine learning tools in histopathology."</blockquote>
                
                <h4>Authors</h4>
                <p class="center-p" style="color: var(--text-secondary);">[List of Authors will be populated here upon publication]</p>
                
                <h4>Affiliations</h4>
                <div class="team-grid">
                    <div class="team-member"><div class="name">1. University of Toronto</div><span class="affiliation">Dept. of Laboratory Medicine and Pathobiology</span></div>
                    <div class="team-member"><div class="name">2. Princess Margaret Cancer Centre</div></div>
                    <div class="team-member"><div class="name">3. University of Toronto</div><span class="affiliation">Dept. of Medical Biophysics</span></div>
                    <div class="team-member"><div class="name">4. University Health Network</div><span class="affiliation">Laboratory Medicine Program, Dept. of Pathology</span></div>
                </div>

                <div style="text-align: center; margin-top: 40px;">
                     <a href="#" class="btn btn-primary" style="background-color: #555; pointer-events: none;">Link to Publication (Coming Soon)</a>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>OnSight is provided as a free tool for research and educational purposes. Not for clinical diagnostic use.</p>
            <p>Developed at the University of Toronto & University Health Network, Toronto, Canada.</p>
            <p>&copy; 2025 Phedias Diamandis, et al. All Rights Reserved.</p>
        </div>
    </footer>

</body>
</html>
